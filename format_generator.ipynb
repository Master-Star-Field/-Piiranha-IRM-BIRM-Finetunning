{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5175293",
   "metadata": {},
   "source": [
    "**Генерация датасета**\n",
    "\n",
    "В отличии от файла *create_dataset_env.ipynb* здесь генерируются данные по среде формата сохранения логов с одинаковым\n",
    "числом логов для каждой подсреды(json, xml и т.д.)\n",
    "\n",
    "Здесь меньше вариантивность в датасете все персональные данные будут на английском языке и уменьшено число вариантов для форматов выдаваемых по типу сервера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8aa7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора: iiiorg/piiranha-v1-detect-personal-information...\n",
      "Токенизатор успешно загружен.\n",
      "\n",
      "Начало генерации структурированного датасета...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Формат: xml_record:   0%|          | 1/1000 [09:00<150:06:28, 540.93s/it]\n",
      "Формат: json_log: 100%|██████████| 1000/1000 [00:05<00:00, 187.03it/s]\n",
      "Формат: xml_record: 100%|██████████| 1000/1000 [00:05<00:00, 176.77it/s]\n",
      "Формат: csv_line: 100%|██████████| 1000/1000 [00:02<00:00, 413.31it/s]\n",
      "Формат: key_value_config: 100%|██████████| 1000/1000 [00:03<00:00, 318.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Генерация завершена. Всего создано 4000 семплов.\n",
      "\n",
      "Датасет успешно сохранен в файл: generated_pii_dataset_structured.csv\n",
      "\n",
      "Статистика по сгенерированным данным:\n",
      "format_type\n",
      "json_log            1000\n",
      "key_value_config    1000\n",
      "csv_line            1000\n",
      "xml_record          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Пример сгенерированной строки (json_log) ---\n",
      "\n",
      "Токены:\n",
      "▁{ ▁\" ▁user _ name ▁\" ▁ : ▁\" ▁ Jennifer ▁\" ▁\" ▁Adams ▁\" ▁ , ▁\" ▁contact _ email ▁\" ▁ : ▁\" ▁ wend y 37 @ example . net ▁\" ▁ , ▁\" ▁phone ▁\" ▁ : ▁\" ▁205 263 2143 ▁\" ▁ , ▁\" ▁last _ login _ ip ▁\" ▁ : ▁\" ▁140 . 248. 225 . 132 ▁\" ▁ , ▁\" ▁home _ address ▁\" ▁ : ▁\" ▁68 184 ▁\" ▁\" ▁ Ricardo ▁\" ▁\" ▁ Through way ▁\" ▁\" ▁Thompson berg , ▁\" ▁\" ▁SD ▁\" ▁\" ▁90 102 ▁\" ▁}\n",
      "\n",
      "Метки:\n",
      "O O O O O O O O O B-PERSON I-PERSON O O I-PERSON O O O O O O O O O O O B-EMAIL I-EMAIL I-EMAIL I-EMAIL I-EMAIL I-EMAIL I-EMAIL I-EMAIL O O O O O O O O O B-PHONE I-PHONE I-PHONE O O O O O O O O O O O O O B-IP_ADDRESS I-IP_ADDRESS I-IP_ADDRESS I-IP_ADDRESS I-IP_ADDRESS I-IP_ADDRESS O O O O O O O O O O O B-ADDRESS I-ADDRESS O O I-ADDRESS I-ADDRESS O O I-ADDRESS I-ADDRESS I-ADDRESS O O I-ADDRESS I-ADDRESS I-ADDRESS O O I-ADDRESS O O I-ADDRESS I-ADDRESS O O\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "NUM_SAMPLES_PER_FORMAT = 1000\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.8\n",
    "OUTPUT_FILE = 'generated_pii_dataset_structured.csv'\n",
    "\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# --- Загрузка токенизатора ---\n",
    "MODEL_NAME = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "print(f\"Загрузка токенизатора: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Токенизатор успешно загружен.\")\n",
    "\n",
    "\n",
    "def create_labels(text: str, base_label: str) -> tuple[str, str]:\n",
    "    words = text.split()\n",
    "    if not words: return \"\", \"\"\n",
    "    labels = [f\"B-{base_label}\"] + [f\"I-{base_label}\"] * (len(words) - 1)\n",
    "    return text, \" \".join(labels)\n",
    "\n",
    "def gen_account_num(): return create_labels(fake.bban(), \"ACCOUNTNUM\")\n",
    "def gen_building_num(): return create_labels(fake.building_number(), \"BUILDINGNUM\")\n",
    "def gen_city(): return create_labels(fake.city(), \"CITY\")\n",
    "def gen_credit_card(): return create_labels(fake.credit_card_number(), \"CREDITCARDNUMBER\")\n",
    "def gen_dob(): return create_labels(fake.date_of_birth().isoformat(), \"DATEOFBIRTH\")\n",
    "def gen_driver_license(): return create_labels(fake.license_plate(), \"DRIVERLICENSENUM\")\n",
    "def gen_email(): return create_labels(fake.email(), \"EMAIL\")\n",
    "def gen_given_name(): return create_labels(fake.first_name(), \"GIVENNAME\")\n",
    "def gen_id_card(): return create_labels(fake.ssn(), \"IDCARDNUM\")\n",
    "def gen_password(): return create_labels(fake.password(), \"PASSWORD\")\n",
    "def gen_social_num(): return create_labels(fake.ssn(), \"SOCIALNUM\")\n",
    "def gen_street(): return create_labels(fake.street_name(), \"STREET\")\n",
    "def gen_surname(): return create_labels(fake.last_name(), \"SURNAME\")\n",
    "def gen_tax_num(): return create_labels(fake.itin(), \"TAXNUM\")\n",
    "def gen_telephone_num(): return create_labels(fake.phone_number(), \"TELEPHONENUM\")\n",
    "def gen_username(): return create_labels(fake.user_name(), \"USERNAME\")\n",
    "def gen_zipcode(): return create_labels(fake.zipcode(), \"ZIPCODE\")\n",
    "\n",
    "def gen_full_name(): return create_labels(fake.name(), \"PERSON\")\n",
    "def gen_address(): return create_labels(fake.address().replace('\\n', ' '), \"ADDRESS\")\n",
    "def gen_phone_number(): return create_labels(fake.phone_number(), \"PHONE\")\n",
    "def gen_ipv4(): return create_labels(fake.ipv4(), \"IP_ADDRESS\")\n",
    "def gen_url(): return create_labels(fake.url(), \"URL\")\n",
    "def gen_timestamp(): return create_labels(fake.iso8601(), \"O\")\n",
    "def gen_status(): return create_labels(random.choice([\"SUCCESS\", \"FAILURE\", \"PENDING\"]), \"O\")\n",
    "def gen_request_method(): return create_labels(random.choice([\"GET\", \"POST\", \"PUT\", \"DELETE\"]), \"O\")\n",
    "\n",
    "def generate_user_profile_content():\n",
    "    return {\n",
    "        \"user_name\": gen_full_name(),\n",
    "        \"contact_email\": gen_email(),\n",
    "        \"phone\": gen_phone_number(),\n",
    "        \"last_login_ip\": gen_ipv4(),\n",
    "        \"home_address\": gen_address(),\n",
    "    }\n",
    "\n",
    "def generate_server_log_content():\n",
    "    return {\n",
    "        \"timestamp\": gen_timestamp(),\n",
    "        \"client_ip\": gen_ipv4(),\n",
    "        \"request\": gen_request_method(),\n",
    "        \"requested_url\": gen_url(),\n",
    "        \"user_email\": gen_email(),\n",
    "        \"status\": gen_status()\n",
    "    }\n",
    "\n",
    "def generate_full_pii_profile():\n",
    "    return {\n",
    "        \"ACCOUNTNUM\": gen_account_num(),\n",
    "        \"BUILDINGNUM\": gen_building_num(),\n",
    "        \"CITY\": gen_city(),\n",
    "        \"CREDITCARDNUMBER\": gen_credit_card(),\n",
    "        \"DATEOFBIRTH\": gen_dob(),\n",
    "        \"DRIVERLICENSENUM\": gen_driver_license(),\n",
    "        \"EMAIL\": gen_email(),\n",
    "        \"GIVENNAME\": gen_given_name(),\n",
    "        \"IDCARDNUM\": gen_id_card(),\n",
    "        \"PASSWORD\": gen_password(),\n",
    "        \"SOCIALNUM\": gen_social_num(),\n",
    "        \"STREET\": gen_street(),\n",
    "        \"SURNAME\": gen_surname(),\n",
    "        \"TAXNUM\": gen_tax_num(),\n",
    "        \"TELEPHONENUM\": gen_telephone_num(),\n",
    "        \"USERNAME\": gen_username(),\n",
    "        \"ZIPCODE\": gen_zipcode(),\n",
    "    }\n",
    "\n",
    "\n",
    "def render_as_json(content: dict) -> tuple[list, list]:\n",
    "    tokens, labels = [\"{\"], [\"O\"]\n",
    "    items = list(content.items())\n",
    "    for i, (key, (value_text, value_labels)) in enumerate(items):\n",
    "        tokens.extend(['\"', key, '\"', ':'])\n",
    "        labels.extend([\"O\", \"O\", \"O\", \"O\"])\n",
    "        value_tokens = value_text.split()\n",
    "        value_labels_split = value_labels.split()\n",
    "        for vt, vl in zip(value_tokens, value_labels_split):\n",
    "            tokens.extend(['\"', vt, '\"'])\n",
    "            labels.extend([\"O\", vl, \"O\"])\n",
    "        if i < len(items) - 1:\n",
    "            tokens.append(\",\")\n",
    "            labels.append(\"O\")\n",
    "    tokens.append(\"}\")\n",
    "    labels.append(\"O\")\n",
    "    return tokens, labels\n",
    "\n",
    "def render_as_xml(content: dict) -> tuple[list, list]:\n",
    "    tokens, labels = [\"<\", \"data\", \">\"], [\"O\", \"O\", \"O\"]\n",
    "    for key, (value_text, value_labels) in content.items():\n",
    "        tokens.extend([\"<\", key, \">\"])\n",
    "        labels.extend([\"O\", \"O\", \"O\"])\n",
    "        value_tokens = value_text.split()\n",
    "        value_labels_split = value_labels.split()\n",
    "        tokens.extend(value_tokens)\n",
    "        labels.extend(value_labels_split)\n",
    "        tokens.extend([\"<\", \"/\", key, \">\"])\n",
    "        labels.extend([\"O\", \"O\", \"O\", \"O\"])\n",
    "    tokens.extend([\"<\", \"/\", \"data\", \">\"])\n",
    "    labels.extend([\"O\", \"O\", \"O\", \"O\"])\n",
    "    return tokens, labels\n",
    "\n",
    "def render_as_csv(content: dict) -> tuple[list, list]:\n",
    "    header = list(content.keys())\n",
    "    tokens, labels = [], []\n",
    "    for i, (key, (value_text, value_labels)) in enumerate(content.items()):\n",
    "        value_text_no_space = value_text.replace(\" \", \"_\")\n",
    "        tokens.append(value_text_no_space)\n",
    "        labels.append(value_labels.split()[0])\n",
    "        if i < len(content) - 1:\n",
    "            tokens.append(\",\")\n",
    "            labels.append(\"O\")\n",
    "    header_tokens = (\",\".join(header) + \" \\n \").split()\n",
    "    tokens = header_tokens + tokens\n",
    "    labels = [\"O\"] * len(header_tokens) + labels\n",
    "    return tokens, labels\n",
    "\n",
    "def render_as_key_value(content: dict) -> tuple[list, list]:\n",
    "    tokens, labels = [], []\n",
    "    for key, (value_text, value_labels) in content.items():\n",
    "        tokens.extend([key, \"=\"])\n",
    "        labels.extend([\"O\", \"O\"])\n",
    "        value_tokens = value_text.split()\n",
    "        value_labels_split = value_labels.split()\n",
    "        tokens.extend(value_tokens)\n",
    "        labels.extend(value_labels_split)\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "FORMAT_CONFIG = {\n",
    "    \"json_log\": {\n",
    "        \"renderer\": render_as_json,\n",
    "        \"content_generators\": [generate_user_profile_content, generate_server_log_content, generate_full_pii_profile]\n",
    "    },\n",
    "    \"xml_record\": {\n",
    "        \"renderer\": render_as_xml,\n",
    "        \"content_generators\": [generate_user_profile_content, generate_full_pii_profile]\n",
    "    },\n",
    "    \"csv_line\": {\n",
    "        \"renderer\": render_as_csv,\n",
    "        \"content_generators\": [generate_server_log_content, generate_user_profile_content, generate_full_pii_profile]\n",
    "    },\n",
    "    \"key_value_config\": {\n",
    "        \"renderer\": render_as_key_value,\n",
    "        \"content_generators\": [generate_server_log_content, generate_full_pii_profile]\n",
    "    }\n",
    "}\n",
    "\n",
    "def tokenize_and_align_labels(tokens: list, labels: list, tokenizer: AutoTokenizer):\n",
    "    assert len(tokens) == len(labels), f\"Tokens and labels length mismatch: {len(tokens)} vs {len(labels)}\"\n",
    "    subword_tokens = []\n",
    "    aligned_labels = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        if not tokenized:\n",
    "            continue\n",
    "        subword_tokens.extend(tokenized)\n",
    "        if label.startswith(\"B-\"):\n",
    "            aligned_labels.append(label)\n",
    "            aligned_labels.extend([\"I-\" + label[2:]] * (len(tokenized) - 1))\n",
    "        else:\n",
    "            aligned_labels.extend([label] * len(tokenized))\n",
    "    return subword_tokens, aligned_labels\n",
    "\n",
    "all_samples = []\n",
    "print(\"\\nНачало генерации структурированного датасета...\")\n",
    "\n",
    "for format_name, config in FORMAT_CONFIG.items():\n",
    "    pbar = tqdm(total=NUM_SAMPLES_PER_FORMAT, desc=f\"Формат: {format_name}\")\n",
    "    renderer = config[\"renderer\"]\n",
    "    content_generators = config[\"content_generators\"]\n",
    "\n",
    "    for _ in range(NUM_SAMPLES_PER_FORMAT):\n",
    "        content_generator = random.choice(content_generators)\n",
    "        content_pack = content_generator()\n",
    "        word_tokens, word_labels = renderer(content_pack)\n",
    "        assert len(word_tokens) == len(word_labels), f\"Mismatch before subword tokenization: {len(word_tokens)} vs {len(word_labels)}\"\n",
    "        subword_tokens, aligned_labels = tokenize_and_align_labels(word_tokens, word_labels, tokenizer)\n",
    "        assert len(subword_tokens) == len(aligned_labels), f\"Mismatch after subword tokenization: {len(subword_tokens)} vs {len(aligned_labels)}\"\n",
    "        all_samples.append({\n",
    "            \"text\": \" \".join(word_tokens),\n",
    "            \"mbert_tokens\": \" \".join(subword_tokens),\n",
    "            \"mbert_token_classes\": \" \".join(aligned_labels),\n",
    "            \"format_type\": format_name,\n",
    "        })\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "print(f\"\\nГенерация завершена. Всего создано {len(all_samples)} семплов.\")\n",
    "\n",
    "\n",
    "if not all_samples:\n",
    "    print(\"Не удалось сгенерировать ни одного семпла.\")\n",
    "else:\n",
    "    df = pd.DataFrame(all_samples)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    split_index = int(len(df) * TRAIN_TEST_SPLIT_RATIO)\n",
    "    df['split'] = np.where(df.index < split_index, 'train', 'test')\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nДатасет успешно сохранен в файл: {OUTPUT_FILE}\")\n",
    "    print(\"\\nСтатистика по сгенерированным данным:\")\n",
    "    print(df['format_type'].value_counts())\n",
    "    print(\"\\n--- Пример сгенерированной строки (json_log) ---\")\n",
    "    sample_row = df[df['format_type'] == 'json_log'].iloc[0]\n",
    "    print(f\"\\nТокены:\\n{sample_row['mbert_tokens']}\")\n",
    "    print(f\"\\nМетки:\\n{sample_row['mbert_token_classes']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
